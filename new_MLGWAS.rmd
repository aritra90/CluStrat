--------------------------------------------------------------------------
Background
--------------------------------------------------------------------------

The goal of the project is to explore a variety of machine learning methods
that can properly distinguish cases from controls in regards to a particular
disease/disorder given genetic data.

As we know, this was the main goal when GWAS studies were being developed.
However, the novel finding of ancestry inference from these studies was
found and explored instead.

We want to find a sophisticated 'method X' that is robust amongst different datasets with
the hope that the method would be able to work well within the same disease
domain (e.g. neurological).

Within the first couple of axes from PCA lie the geographical information that
is encoded in our genetics. This is why GWAS studies have focused on the ancestry.
However, there are axes in the PCA result that infer disease but of course they are
much more difficult to find and interpret.

--------------------------------------------------------------------------
PreReqs
--------------------------------------------------------------------------

>> module load gcc
>> module load bioinfo
>> module load plink
>> module load texlive

The '.bed' files for the particular dataset that you are using are needed.

Also, the top association SNPs from GWAS pipeline are needed (at least 2000 SNPs).

--------------------------------------------------------------------------
Command Line Arguments
--------------------------------------------------------------------------
**Running the script**
>> qsub -v dataset_prefix=dataset_prefix,extract=yes/no,type=LDA/SVM/Ridge...,cv=kfold/loocv,help=yes/no new_GWAS_ML_script.sh

**e.g.**
>> qsub -v dataset_prefix=SZHCGSRRS,type=LDA,cv=loocv new_GWAS_ML_script.sh

Use the 'extract' option if the data has not been split into testing and training yet.
It is safe to always use the extract option but might take longer as datasets get larger.

The 'type' flag is necessary to denote what algorithm to use.

The 'cv' flag is to set what type of cross-validation to use. 5-fold is used by default.

The 'help' flag being set to 'yes' will exit the program and echo the proper usage.

Class Options = [LDA,QDA,PolyReg,Ridge,Lasso,Elastic,SVM,SVR,kSVM,RidgeSVM,RFESVM,RandomForest]

The output of a particular run (one algorithm and one type of cross-validation scheme for
many different number of top SNPs to include) will be the following:

.out file --> any print messages from running the script (PLINK command output, result statistics,
    confusion matrices, etc.)

directory --> a directory will be made to place all of the files from 'DIMdetect2.py' which are mainly
    the '.pdf' and '.tex' files.

.txt files --> many text files including the accuracies, f1 scores, parameters etc. will be generated
    for plots.

Any flag that is not specified and not mandatory (dataset_prefix) will be set to defaults and
those options will be echoed.

**Plotting**
To plot and get the full pdf, run the following:

>> python fullDoc.py --data PRK/SCZ

The 'data' flag is used to plot figures after running different algorithms for a given dataset.
For example, after running the GWAS_ML_script for Parkinsons using LDA/Ridge/SVM (3 different runs),
then running the plot file will plot accuracy and f1score lines for each of the 3 algorithms.

All of the output files will be prefixed by the dataset used, # of SNPs extracted and the datetime.
(except the '.txt' files used for plotting)

**Cleaning**
>> ./clean.sh

This script will remove most of the output files generated by the scripts just to clear clutter.

In regards to the parameters and accuracy, I am currently looping through different sets of parameters
for each value of extracted SNPs and then storing the best accuracy and parameter values. This is a bit
problematic because when comparing the performance of other algorithms whose best accuracy was gotten using
a different set of parameters, it can be inconsistent. However, currently I am only using the default set of
parameters for each algorithm.

*possibly add kernel Ridge??*

--------------------------------------------------------------------------
Design Decisions and Issues
--------------------------------------------------------------------------
11/7/2018:
Running into a lot of pre-req packaging errors. Trying the following:

**Solution: Run the 'download_requirements.sh' script.**

**Solution: Run the following commands:**

>>pip freeze > old.pkgs
>>module purge && module load <some compiler>
>>pip install -r old.pkgs

**Solution:**
>>pip install --user plinkio

**FINAL SOLUTION:**
>> module load gcc

THEN install the package (uninstall it if you have it already and
reinstall with the new compiler)

Also, make sure you are using the file prefix if getting any PLINK or
other file errors. For example, dont use "PRKXXXX.bed" but use
"PRKXXXX".

11/13/2018:
Not a lot of parameters to manage with the sklearn LDA so not saving any
parameters using 'pickle' package etc.

Planning to apply 'vanilla' versions of all the packages and clear up any
errors.

11/14/2018:
**SOLUTION:** All the metric errors seem to be a biproduct of class imbalance in the cross
validation step but we need to understand this error in other contexts and how
it affects results downstream if at all.


-------------------------------------------------------------------------------
LEGEND:
#Algorithm still has an error
*Algorithm has solutions to all the found errors*
**Solutions to the algorithm errors**
-------------------------------------------------------------------------------


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#LDA Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.
  warnings.warn("Variables are collinear.")

Only get this error with high number of pvals extracted (1000 or more).

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#QDA Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:686: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")
(2) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

Resources:
https://stats.stackexchange.com/questions/29385/collinear-variables-in-multiclass-lda-training
https://stackoverflow.com/questions/43162506/undefinedmetricwarning-f-score-is-ill-defined-and-being-set-to-0-0-in-labels-wi

The collinearity of variables seems like a property of the data so I am not currently sure how to
go about changing this. Essentially, the algorithm can't sense or differentiate each variables'
contribution to the problem but in my mind that just opens the problem to non-unique solutions or
multiple decision boundaries but for prediction this should be fine.

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

*Polynomial Regression Errors:*
(1) scores, model = cross_validation_custom(G_train, train_pheno,model_init)
 File "DIMdetect2.py", line 46, in cross_validation_custom
   X_train = poly.fit_transform(X_train)
 File "/home/mcburch/.local/lib/python2.7/site-packages/sklearn/base.py", line 462, in fit_transform
   return self.fit(X, fit_params).transform(X)
 File "/home/mcburch/.local/lib/python2.7/site-packages/sklearn/preprocessing/data.py", line 1478, in transform
   XP = np.empty((n_samples, self.n_output_features_), dtype=X.dtype)
MemoryError

**SOLUTION: Lower the degree of features to model.**
Higher degree causing a lot of issues for some reason...

Resources:
https://stats.stackexchange.com/questions/58739/polynomial-regression-using-scikit-learn
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html

Also, this method takes a very long time compared to the others. I believe this is the case because
the data gets transformed to a larger feature space and that is expensive to train.

Had to comment out the line that fills the report (now uncommented). I assume something I have altered for the algorithm
has "upset" the results. A lot of other errors coming from the fact that this is a **regression method
that yielded continuous predictions instead of binary ones which cause a lot of errors so I had to
discretize the predictions** to get it functioning again. In the future, I might need to change the boundary
on discretization (relax the bound a bit).

Degrees higher than 2 take a while so submit as jobs.

Also takes a while with more pvals to be extracted. Not quite sure how to manage the time here.

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

*Ridge Errors:*
(1) Traceback (most recent call last):
  File "DIMdetect2.py", line 155, in <module>
    scores, model = cross_validation_custom(G_train, train_pheno,model_init)
  File "DIMdetect2.py", line 38, in cross_validation_custom
    score += accuracy_score(preds, y_test)
  File "/home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py", line 176, in accuracy_score
    y_type, y_true, y_pred = check_targets(y_true, y_pred)
  File "/home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py", line 81, in check_targets
    "and {1} targets".format(type_true, type_pred))
ValueError: Classification metrics can't handle a mix of continuous and binary targets

**SOLUTION: Need to change continuous return values of 'model.predict' to binary. Fixed with 'round()' method.**

(2) Traceback (most recent call last):
  File "DIMdetect2.py", line 160, in <module>
    scores, model = cross_validation_custom(G_train, train_pheno,model_init)
  File "DIMdetect2.py", line 50, in cross_validation_custom
    conf += get_conf(model, X_test, y_test)
ValueError: operands could not be broadcast together with shapes (4,4) (3,3) (4,4)

**SOLUTION: Commented out a strange concatenations of confusion matrices.**

(3) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.
  'recall', 'true', average, warn_for)

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#Lasso Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

"ill-defined" scores more often than not so the results are not accurate.

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#Elastic Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

*SVM Errors:*


-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#kSVM Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

#RidgeSVM Errors:
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

*RFESVM Errors:*
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

Also, this method takes a *very long time* compared to the others.

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

*Random Forest Errors:*
(1) /home/mcburch/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)

-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------

**SOLUTION: In regards to the 'UndefinedMetricWarnings', the warnings only occur when there
are labels that have no predicted samples after predicting with the model. In this case, this
means that either the cases or controls are all being properly predicted and the other is not.
Since we only have binary classes (has disease or not), either the models with this warning are
failing and thus resorting to a simple binary classifier or they are truly only predicting one class.
This is an issue because the f1score divides two of the metrics that would be 0 due to this issue.
A possible solution to this is making sure that each model uses balanced class weights so that this
binary classification doesn't occur.**


11/16/2018:
Implementing parameter saving as well as cross validation flag (k-fold or leave one out).
Only error remaining is variable collinearity with QDA.

11/26/2018:
Fixed the issue of the extracting using hapmap. Must not have used to the correct
format earlier.


https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62


12/23/2018:
Bad confusion matrices solved by properly rounding the predicted classifications.
The learners were sometimes creating more than the 2 original classes.

Many edits were made to 'DIMdetect2.py' and 'report_generator.py' as well.

1/3/2019:
Fixed some plotting issues so that it runs a bit smoother.
Also, fixed the parametrized version of the script.

Running the scripts with LOOCV as a job which will take much longer.
Fixing other algorithm issues in the meantime.

Edited 'bose_splitdata.sh' to incorporate subsampling and hopefully fix
metric errors.

1/8/2019:
Added flag for subsampling.

Edited some naming of files.

Looking into report and output file discrepancies (SOLVED).
**SOLUTION: Confusion matrix documentation from scikit was different then what
I originally coded.**

1/14/2019:
Plotting sensitivity and specificity and finalizing reports.

--------------------------------------------------------------------------
End of ReadMe
--------------------------------------------------------------------------
